{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cdda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from scipy.special import softmax \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs \n",
    "from sklearn.preprocessing import scale\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ecdist\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import requests, zipfile, io\n",
    "import random\n",
    "# from sklearn.cluster.k_means_ import _init_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cca693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizefea(X):\n",
    "    \"\"\"\n",
    "    X: stacked feature data matrix\n",
    "    return: L2 normalized data matrix\n",
    "    \"\"\"\n",
    "    feanorm = np.maximum(1e-14,np.sum(X**2,axis=1))\n",
    "    X_out = X/(feanorm[:,None]**0.5)\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1844f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def km_le(X,M):\n",
    "    \n",
    "    \"\"\"\n",
    "    X: stacked point matrix\n",
    "    M: stacked center matrix\n",
    "    return: point assignment (clustering labels) to their closest centers\n",
    "    \n",
    "    \"\"\"\n",
    "    e_dist = ecdist(X,M)          \n",
    "    l = e_dist.argmin(axis=1)\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373081d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class MiniBatchKMeans: implement the classical mini-batch k-means algorithm from scratch\n",
    "'''\n",
    "class MiniBatchKMeans:\n",
    "    n_clusters = 2\n",
    "    dim_n = 2\n",
    "    init_centroids = None\n",
    "    init_labels = None\n",
    "    final_centroids = None\n",
    "    final_labels = None\n",
    "    \n",
    "    def __init__(self, name, n_samples = 400, seed = 1):\n",
    "        ## generate synthetic data or load real dataset\n",
    "        if name == 'Synthetic-equal':\n",
    "            centers = [(1, 1), (2.1, 1), (1, 5), (2.1, 5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=n_samples, n_features=self.dim_n, cluster_std=0.1,\n",
    "                      centers=centers, shuffle=False, random_state=1)\n",
    "\n",
    "            index = n_samples // 2\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:n_samples] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "\n",
    "        elif name == 'Synthetic2-equal':\n",
    "            sample_list = [200,200]\n",
    "            centers = [(1, 1), (2.1, 3.5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=sample_list, n_features=self.dim_n, cluster_std=0.3, \\\n",
    "                                       centers=centers, shuffle=False, random_state=seed)\n",
    "\n",
    "            index = sample_list[0]\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "    \n",
    "        elif name == 'Synthetic-unequal':\n",
    "            sample_list = [150,150,50,50]\n",
    "            centers = [(1, 1), (2.1, 1), (1, 3.5), (2.1, 3.5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=sample_list, n_features=self.dim_n, cluster_std=0.13, \\\n",
    "                                   centers=centers, shuffle=False, random_state=seed)\n",
    "\n",
    "            index = sample_list[0]+sample_list[1]\n",
    "\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "            \n",
    "        elif name == 'Synthetic2-unequal': \n",
    "            sample_list = [300,100]\n",
    "            centers = [(1, 1), (2.1, 3.5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=sample_list, n_features=self.dim_n, cluster_std=0.3, \\\n",
    "                                   centers=centers, shuffle=False, random_state=seed)\n",
    "            index = sample_list[0]\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "\n",
    "        elif name == 'Adult':\n",
    "            data_dir = \"data/\"\n",
    "            savepath = osp.join(data_dir, name+'.npz')\n",
    "\n",
    "            datas = np.load(savepath)\n",
    "            X_org = datas['X_org']\n",
    "            demograph = datas['demograph']\n",
    "            K = datas['K'].item()\n",
    "            \n",
    "            n_samples = len(demograph)\n",
    "            self.data_org, self.gender, K = X_org, demograph, K\n",
    "            self.n_clusters = n_clu\n",
    "            self.dim_n = 5\n",
    "            self.n_demoGroups = len(list(set(self.gender)))\n",
    "\n",
    "            data = scale(self.data_org, axis = 0)\n",
    "            data = normalizefea(data)\n",
    "            \n",
    "        elif name == 'Bank':\n",
    "            data_dir = \"data/\"\n",
    "            savepath = osp.join(data_dir, name+'.npz')\n",
    "            \n",
    "            datas = np.load(savepath)\n",
    "            X_org = datas['X_org']\n",
    "            demograph = datas['demograph']\n",
    "            K = datas['K'].item()\n",
    "            \n",
    "            n_samples = len(demograph)\n",
    "            self.data_org, self.gender, K = X_org, demograph, K\n",
    "            self.n_clusters = n_clu\n",
    "            self.dim_n = 6\n",
    "            self.n_demoGroups = len(list(set(self.gender)))\n",
    "\n",
    "            data = scale(self.data_org, axis = 0)\n",
    "            data = normalizefea(data)\n",
    "\n",
    "        \n",
    "        if name in ['Adult', 'Bank']: \n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            \n",
    "            n_samples = 5000\n",
    "            numbers = np.random.choice(len(self.gender), size=n_samples, replace=False)\n",
    "            \n",
    "            self.gender = self.gender[numbers]\n",
    "            self.point_mat = np.zeros([n_samples, self.dim_n + 1])\n",
    "            self.point_mat[:, :self.dim_n] = data[numbers, :self.dim_n]\n",
    "            self.point_mat[:, self.dim_n] = self.gender.astype(int)\n",
    "\n",
    "        else:\n",
    "            self.gender = self.gender[:n_samples]\n",
    "            self.point_mat = np.zeros([n_samples, self.dim_n + 1])\n",
    "            self.point_mat[:, :self.dim_n] = data[:n_samples, 0:self.dim_n]\n",
    "            self.point_mat[:, self.dim_n] = self.gender.astype(int)\n",
    "        \n",
    "        self.data_name = name\n",
    "        self.n_samples = n_samples\n",
    "        self.max_iters = self.n_samples//10\n",
    "    \n",
    "    def initial_clustering(self, seed = 1):\n",
    "        np.random.seed(seed)\n",
    "        self.init_centroids = self.point_mat[np.random.randint(self.n_samples, size = self.n_clusters), 0:self.dim_n]*1.0\n",
    "        self.init_labels = self.regular_assignment(self.point_mat[:, 0:self.dim_n], self.init_centroids)\n",
    "        \n",
    "    ## define the regular assignment function: assign to the closest point\n",
    "    def regular_assignment(self, pt, centroids):\n",
    "        arr_dist = np.zeros([len(pt), self.n_clusters])\n",
    "        for i in range(self.n_clusters):\n",
    "            arr_dist[:, i] =  np.linalg.norm(pt - centroids[i, :], axis = 1)\n",
    "        label = np.argmin(arr_dist, axis = 1)\n",
    "        return label\n",
    "\n",
    "    def plot_demo(self):\n",
    "        colmap_demo = {0: 'k', 1: 'm', 2: 'g', 3:'y', 4:'DarkGreen', 5:'r', 6:'b', 7:'c', 8: 'LightBlue', 9:\"Orange\"}\n",
    "        colors = list(map(lambda x: colmap_demo[x], self.gender))\n",
    "        plt.scatter(self.point_mat[:, 0], self.point_mat[:, 1], color=colors, alpha=0.5, edgecolor='k')\n",
    "        plt.title(\"Demograpic composition: black->males, purple->females\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_initClu(self):\n",
    "        colmap_clu = {0: 'r', 1: 'b', 2: 'g', 3:'y', 4:'DarkGreen', 5:'k', 6:'m', 7:'c', 8: 'LightBlue', 9:\"Orange\"}\n",
    "        colors = list(map(lambda x: colmap_clu[x], self.init_labels))\n",
    "        \n",
    "        fig2 = plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(self.point_mat[:, 0], self.point_mat[:, 1], color=colors, alpha=0.3, edgecolor='k')\n",
    "        for i in range(self.n_clusters):\n",
    "            plt.scatter(self.init_centroids[i, 0], self.init_centroids[i, 1], color=colmap_clu[i])\n",
    "        plt.title(\"Initial clustering using initial centroids: red vs blue\")\n",
    "        plt.show()\n",
    "    \n",
    "    def update_objective_f1(self, centroids, arr_label):\n",
    "        sum_dist = 0\n",
    "        for i in range(self.n_clusters):\n",
    "            idx = np.where(arr_label == i)[0]\n",
    "            data_c1 = self.point_mat[idx, 0:self.dim_n]\n",
    "            sum_dist += np.sum(np.linalg.norm(data_c1 - centroids[i, :], axis = 1)**2)\n",
    "        return sum_dist/len(self.point_mat)\n",
    "    \n",
    "    def plot_FinalClu(self):\n",
    "        colmap_clu = {0: 'r', 1: 'b', 2: 'g', 3:'y', 4:'DarkGreen', 5:'k', 6:'m', 7:'c', 8: 'LightBlue', 9:\"Orange\"}\n",
    "        colors = list(map(lambda x: colmap_clu[x], self.final_labels))\n",
    "\n",
    "        fig3 = plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(self.point_mat[:, 0], self.point_mat[:, 1], color=colors, alpha=0.3, edgecolor='k')\n",
    "        for i in range(self.n_clusters):\n",
    "            plt.scatter(self.final_centroids[i, 0], self.final_centroids[i, 1], color=colmap_clu[i])\n",
    "        plt.title(\"Final clustering using final centroids: red vs blue\")\n",
    "        plt.show()\n",
    "\n",
    "    def miniBatchKmeans(self, max_iters = 400, batch_size = 10):\n",
    "        centroids = self.init_centroids\n",
    "        \n",
    "        arr_label = np.zeros(self.n_samples) # self.init_labels\n",
    "        cluster_size = np.zeros(self.n_clusters)\n",
    "        \n",
    "        arr_f1 = np.zeros(max_iters)\n",
    "        iters = 0\n",
    "        while iters < max_iters: # or len(np.where(arr_label == 0)[0]) > 10\n",
    "            batch_index = np.random.choice(self.n_samples, size=batch_size, replace=False)\n",
    "            batch_matrix = self.point_mat[batch_index, 0:self.dim_n]\n",
    "            batch_labels = self.regular_assignment(batch_matrix, centroids)\n",
    "            \n",
    "            ## record old labels and assign new labels\n",
    "            old_label = arr_label[batch_index]\n",
    "            arr_label[batch_index] = batch_labels\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                label = batch_labels[i]\n",
    "                oldlabel= int(old_label[i])\n",
    "                \n",
    "                if oldlabel == 0:\n",
    "                    cluster_size[label] += 1\n",
    "                elif oldlabel != label:\n",
    "                    cluster_size[oldlabel] -= 1\n",
    "                    cluster_size[label] += 1\n",
    "                \n",
    "                if oldlabel != label:\n",
    "                    ita = 1.0/cluster_size[label] \n",
    "                    centroids[label] = centroids[label]*(1.0 - ita) + batch_matrix[i, :]*ita\n",
    "                    \n",
    "            updated_labs = self.regular_assignment(self.point_mat[:, 0:self.dim_n], centroids)\n",
    "            arr_f1[iters] = self.update_objective_f1(centroids, updated_labs)\n",
    "            iters += 1\n",
    "        \n",
    "        self.final_centroids = centroids\n",
    "        self.final_labels = updated_labs\n",
    "        \n",
    "        # plot \n",
    "        plt.plot(arr_f1, 'k.')\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Clustering cost\")\n",
    "        plt.show()\n",
    "        print(\"Final clustering cost\", arr_f1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e79bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchFairKMeans:\n",
    "    \n",
    "    n_clusters = 2\n",
    "    dim_n = 2\n",
    "    n_demoGroups = 2\n",
    "    n_init = 10\n",
    "    \n",
    "    init_provided = False\n",
    "    f1_list = None\n",
    "    f2_list = None\n",
    "    \n",
    "    ## parameters of PF-SAGD algorithm\n",
    "    num_iter = 0\n",
    "    max_iter = 1000\n",
    "    max_len_pareto_front = 1500\n",
    "    \n",
    "    stepsize = 1.0\n",
    "    \n",
    "    point_per_iteration = 3\n",
    "    num_steps_per_point = 1\n",
    "    \n",
    "    dense_threshold = 0\n",
    "    \n",
    "    def __init__(self, name, n_samples = 400, seed = 1, n_clu = 10):\n",
    "        if name == 'Synthetic-equal':\n",
    "            centers = [(1, 1), (2.1, 1), (1, 5), (2.1, 5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=n_samples, n_features=self.dim_n, cluster_std=0.1,\n",
    "                      centers=centers, shuffle=False, random_state=1)\n",
    "\n",
    "            index = n_samples // 2\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:n_samples] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "\n",
    "        elif name == 'Synthetic2-equal':\n",
    "            sample_list = [200,200]\n",
    "            centers = [(1, 1), (2.1, 3.5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=sample_list, n_features=self.dim_n, cluster_std=0.3, \\\n",
    "                                       centers=centers, shuffle=False, random_state=seed)\n",
    "\n",
    "            index = sample_list[0]\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "    \n",
    "        elif name == 'Synthetic-unequal':\n",
    "            sample_list = [150,150,50,50]\n",
    "            centers = [(1, 1), (2.1, 1), (1, 3.5), (2.1, 3.5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=sample_list, n_features=self.dim_n, cluster_std=0.13, \\\n",
    "                                   centers=centers, shuffle=False, random_state=seed)\n",
    "\n",
    "            index = sample_list[0]+sample_list[1]\n",
    "\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "            \n",
    "        elif name == 'Synthetic2-unequal': \n",
    "            sample_list = [300,100]\n",
    "            centers = [(1, 1), (2.1, 3.5)]\n",
    "            self.data_org, self.gender = make_blobs(n_samples=sample_list, n_features=self.dim_n, cluster_std=0.3, \\\n",
    "                                   centers=centers, shuffle=False, random_state=seed)\n",
    "            index = sample_list[0]\n",
    "            self.gender[0:index] = 0\n",
    "            self.gender[index:] = 1\n",
    "            \n",
    "            data = self.data_org\n",
    "\n",
    "        elif name == 'Adult':\n",
    "            data_dir = \"data/\"\n",
    "            savepath = osp.join(data_dir, name+'.npz')\n",
    "\n",
    "            datas = np.load(savepath)\n",
    "            X_org = datas['X_org']\n",
    "            demograph = datas['demograph']\n",
    "            K = datas['K'].item()\n",
    "            \n",
    "            n_samples = len(demograph)\n",
    "            self.data_org, self.gender, K = X_org, demograph, K\n",
    "            self.n_clusters = n_clu\n",
    "            self.dim_n = 5\n",
    "            self.n_demoGroups = len(list(set(self.gender)))\n",
    "\n",
    "            data = scale(self.data_org, axis = 0)\n",
    "            data = normalizefea(data)\n",
    "            \n",
    "        elif name == 'Bank':\n",
    "            data_dir = \"data/\"\n",
    "            savepath = osp.join(data_dir, name+'.npz')\n",
    "            \n",
    "            datas = np.load(savepath)\n",
    "            X_org = datas['X_org']\n",
    "            demograph = datas['demograph']\n",
    "            K = datas['K'].item()\n",
    "            \n",
    "            n_samples = len(demograph)\n",
    "            self.data_org, self.gender, K = X_org, demograph, K\n",
    "            self.n_clusters = n_clu\n",
    "            self.dim_n = 6\n",
    "            self.n_demoGroups = len(list(set(self.gender)))\n",
    "\n",
    "            data = scale(self.data_org, axis = 0)\n",
    "            data = normalizefea(data)\n",
    "\n",
    "        \n",
    "        if self.data_name in ['Adult', 'Bank']: \n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            \n",
    "            n_samples = 5000\n",
    "            numbers = np.random.choice(len(self.gender), size=n_samples, replace=False)\n",
    "            \n",
    "            self.gender = self.gender[numbers]\n",
    "            self.point_mat = np.zeros([n_samples, self.dim_n + 1])\n",
    "            self.point_mat[:, :self.dim_n] = data[numbers, :self.dim_n]\n",
    "            self.point_mat[:, self.dim_n] = self.gender.astype(int)\n",
    "\n",
    "        else:\n",
    "            self.gender = self.gender[:n_samples]\n",
    "            self.point_mat = np.zeros([n_samples, self.dim_n + 1])\n",
    "            self.point_mat[:, :self.dim_n] = data[:n_samples, 0:self.dim_n]\n",
    "            self.point_mat[:, self.dim_n] = self.gender.astype(int)\n",
    "    \n",
    "        self.data_name = name\n",
    "        self.n_samples = n_samples\n",
    "        self.max_iters = self.n_samples\n",
    "        \n",
    "    ## compute distance of the set of point to the set of centers\n",
    "    def distance(self, pt, centroids):\n",
    "        arr_dist = np.zeros([len(pt), self.n_clusters])\n",
    "        for i in range(self.n_clusters):\n",
    "            arr_dist[:, i] =  np.linalg.norm(pt - centroids[i, :], axis = 1)\n",
    "        return arr_dist     \n",
    "    \n",
    "    ## compute center according to a specific label\n",
    "    def update_centers(self, arr_label):\n",
    "        centroids = np.zeros([self.n_clusters, self.dim_n])\n",
    "        for i in range(self.n_clusters):\n",
    "            index_k = np.where(arr_label == i)[0]\n",
    "            if len(index_k) == 0:\n",
    "                continue\n",
    "            centroids[i, :] = np.mean(self.point_mat[index_k, 0:self.dim_n], axis=0)\n",
    "        return centroids\n",
    "    \n",
    "    ## randomly generate num sets of labels\n",
    "    def init_labels_centers(self, seed = 1):\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.arr_init_labels = np.zeros([self.n_init, self.n_samples])*(-1)\n",
    "        self.arr_init_centroids = np.zeros([self.n_init, self.n_clusters, self.dim_n])\n",
    "        rlen = self.n_init // 2\n",
    "        for i in range(rlen):\n",
    "            if self.data_name in ['Adult', 'Bank']:\n",
    "                p = np.random.randn(self.n_clusters)\n",
    "                prob = softmax(p)\n",
    "                init_labels = np.random.choice(self.n_clusters, self.n_samples, p=prob)\n",
    "            else:\n",
    "                p1, p2 = np.random.rand(1)[0], np.random.rand(1)[0]\n",
    "                init_labels = np.append(np.random.choice(self.n_clusters, 300, p=[p1, 1-p1]), np.random.choice(self.n_clusters, 100, p=[p2, 1-p2]))\n",
    "            \n",
    "            init_centroids = self.update_centers(init_labels)\n",
    "            self.arr_init_labels[i, :] = init_labels\n",
    "            self.arr_init_centroids[i, :, :] = init_centroids\n",
    "        \n",
    "        for i in range(rlen, self.n_init):\n",
    "            M =_init_centroids(self.point_mat[:, :self.dim_n], self.n_clusters, init='k-means++')\n",
    "            l = km_le(self.point_mat[:, :self.dim_n], M)\n",
    "            self.arr_init_labels[i, :] = l\n",
    "            self.arr_init_centroids[i, :, :] = M\n",
    "    \n",
    "    def plot_demo(self, arr_labels):\n",
    "        count_demo = np.zeros([self.n_clusters, self.n_demoGroups])\n",
    "        for k in range(self.n_clusters):\n",
    "            for j in range(self.n_demoGroups):\n",
    "                count_demo[k, j] = len(np.where((self.gender == j) & (arr_labels == k))[0]) #zeros in cluster 1\n",
    "         \n",
    "        balance_1 = np.min(count_demo[0, :])/np.max(count_demo[0, :])\n",
    "        balance_2 = np.min(count_demo[1, :])/np.max(count_demo[1, :])\n",
    "\n",
    "        print (\"Balance\", np.min([balance_1, balance_2]))\n",
    "\n",
    "        labels = ['C1', 'C2']\n",
    "        men_means = count_demo[:, 0]\n",
    "        women_means = count_demo[:, 1]\n",
    "\n",
    "        x = np.arange(len(labels))  # the label locations\n",
    "        width = 0.35  # the width of the bars\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(x - width/2, men_means, width, label='Men')\n",
    "        rects2 = ax.bar(x + width/2, women_means, width, label='Women')\n",
    "\n",
    "        # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "        ax.set_ylabel('Numbers')\n",
    "        ax.set_title('Clustering gender distribution')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.legend()\n",
    "\n",
    "        def autolabel(rects):\n",
    "            \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                ax.annotate('{}'.format(height),\n",
    "                            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom')\n",
    "\n",
    "\n",
    "        autolabel(rects1)\n",
    "        autolabel(rects2)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_cluster(self, arr_labels, centroids, savepath = None):\n",
    "        colmap_clu = {0: 'r', 1: 'b', 2: 'g', 3:'y', 4:'DarkGreen', 5:'k', 6:'m', 7:'c', 8: 'LightBlue', 9:\"Orange\"}\n",
    "        colors = list(map(lambda x: colmap_clu[x], arr_labels))   \n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.xaxis.set_tick_params(labelsize=22)\n",
    "        ax.yaxis.set_tick_params(labelsize=22)\n",
    "        ax.set_xlabel('$x_1$', fontsize = 22)\n",
    "        ax.set_ylabel('$x_2$', fontsize = 22)\n",
    "        ax.scatter(self.point_mat[:, 0], self.point_mat[:, 1], color=colors, alpha=0.2, edgecolor='k')\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            ax.scatter(centroids[i, 0], centroids[i, 1], color=colmap_clu[i])\n",
    "#         plt.title(\"Clustering Visulization: red vs blue\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        if savepath:\n",
    "            fig.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "    ## compute demographic compositions for k clusters\n",
    "    def compute_demo(self, arr_labels):\n",
    "        count_demo = np.zeros([self.n_clusters, self.n_demoGroups])\n",
    "        for k in range(self.n_clusters):\n",
    "            for j in range(self.n_demoGroups):\n",
    "                count_demo[k, j] = len(np.where((self.gender == j) & (arr_labels == k))[0])\n",
    "        return count_demo\n",
    "    \n",
    "    ## compute balance\n",
    "    def compute_balance(self, count_demo):\n",
    "        '''\n",
    "        Input\n",
    "            count_demo: demographic compositions for k clusters\n",
    "        \n",
    "        Output:\n",
    "            arr_balance: balance of k clusters\n",
    "            np.min(arr_balance): overall cluster balance\n",
    "            bn_cluster: the cluster index of minimum balance cluster\n",
    "            bn_low: the minority demographic group in cluster bn_cluster\n",
    "            bn_high: the majority demographic group in cluster bn_cluster\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        max_count = np.max(count_demo, axis = 1)\n",
    "        min_count = np.min(count_demo, axis = 1)\n",
    "        arr_balance = min_count/max_count\n",
    "  \n",
    "        bn_cluster = np.argmin(arr_balance)\n",
    "        bn_low = np.argmin(count_demo[bn_cluster, :])\n",
    "        bn_high = np.argmax(count_demo[bn_cluster, :])\n",
    "        \n",
    "        return arr_balance, np.min(arr_balance), bn_cluster, bn_low, bn_high\n",
    "    \n",
    "    ## determine a target well-balanced cluster\n",
    "    def choose_target_cluster(self, bn_cluster, bn_low, bn_high, count_demo, centroids, method = 'global'):\n",
    "        if method == 'global':\n",
    "            ## choose the cluster with the highest ratio between group bn_low and bn_high\n",
    "            eps = 1e-16\n",
    "            biased_cluster = np.argmax(count_demo[:, bn_low]/(count_demo[:, bn_high] + eps))\n",
    "        elif method == 'local':\n",
    "            ## choose the clutser closet to bn_cluster\n",
    "            c1 = centroids[bn_cluster]\n",
    "            dist = np.ones(self.n_clusters)*float('inf')\n",
    "            for i in range(self.n_clusters):\n",
    "                if i == bn_cluster: continue\n",
    "                dist[i] = np.linalg.norm(centroids[i] - c1)\n",
    "            assert np.min(dist) != float('inf')\n",
    "            biased_cluster = np.argmin(dist)\n",
    "        return biased_cluster\n",
    "    \n",
    "    \n",
    "    ## Three clustering functions\n",
    "    '''\n",
    "    curr_label + curr_centers => clustering cost directly\n",
    "    '''\n",
    "    def update_objective_f1(self, arr_label, centroids):\n",
    "        sum_dist = 0\n",
    "        for i in range(self.n_clusters):\n",
    "            idx = np.where(arr_label == i)[0]\n",
    "            data_ci = self.point_mat[idx, 0:self.dim_n]\n",
    "            if len(data_ci) == 0:\n",
    "#                 assert len(data_ci) == 0\n",
    "                continue\n",
    "            sum_dist += np.sum(np.linalg.norm(data_ci - centroids[i, :], axis = 1)**2)\n",
    "#             sum_dist += np.sum(ecdist(data_ci, centroids[i, :].reshape(1, -1), squared =True))\n",
    "        return sum_dist/self.n_samples\n",
    "\n",
    "    '''\n",
    "    current labels => update true centers by average => compute clustering cost according to current labels\n",
    "\n",
    "    '''\n",
    "    def update_objective_f1_trueCenter(self, arr_label):\n",
    "        updatedCenter = np.zeros((self.n_clusters, self.n_samples))\n",
    "        sum_dist = 0.0\n",
    "        for i in range(self.n_clusters):\n",
    "            idx_ci = np.where(arr_label == i)[0]\n",
    "            if len(idx_ci) == 0:\n",
    "                continue\n",
    "            center = np.mean(self.point_mat[idx_ci, 0:self.dim_n], axis = 0, keepdims = True)\n",
    "            sum_dist += np.sum(np.linalg.norm(self.point_mat[idx_ci, 0:self.dim_n] - center, axis = 1)**2)\n",
    "        return sum_dist/self.n_samples\n",
    "\n",
    "    '''\n",
    "    curr_centers => re-assign every points to its closest center => average cost/distance\n",
    "    '''\n",
    "    def update_objective_f1_kmeans(self, centroids):\n",
    "        arr_dist = self.distance(self.point_mat[:, 0:self.dim_n], centroids)\n",
    "        sum_dist = np.mean(np.min(arr_dist, axis = 1))\n",
    "        return sum_dist\n",
    "    \n",
    "    ''' Inexact update of centers for pure kmeans updates'''\n",
    "    def update_centers_kmeans(self, batch_mat, arr_label, curr_centroids, count_demo, alpha):\n",
    "        counter = np.sum(count_demo, axis = 1)\n",
    "        bacth_dist = self.distance(batch_mat, curr_centroids)\n",
    "        closest_center_idx = np.argmin(bacth_dist, axis = 1)\n",
    "        for i in range(len(batch_mat)):\n",
    "            idx = closest_center_idx[i]\n",
    "            counter[idx] = counter[idx] + 1.0\n",
    "            curr_centroids[idx, :] = curr_centroids[idx, :]*(1.0 - alpha/counter[idx]) + batch_mat[i, :]*alpha/counter[idx]\n",
    "        return curr_centroids, closest_center_idx\n",
    "    \n",
    "    ''' Inexact update of centers for pure swap updates'''\n",
    "    def update_centers_balance(self, arr_label, curr_centroids, in_idx, count_demo, alpha):\n",
    "        '''\n",
    "        Input\n",
    "        arr_label：vector of clustering labels\n",
    "        curr_centroids: the set of k centers \n",
    "        in_idx: index of in points for all clusters\n",
    "        count_demo: current demographic compositions for k clusters before swap\n",
    "        alpha: 1.0 by default, use to control the step size of center updates\n",
    "        \n",
    "        Output\n",
    "        curr_centroids: the new set of k centers \n",
    "        '''\n",
    "        counter = np.sum(count_demo, axis = 1)\n",
    "        for i in range(self.n_clusters):\n",
    "            if len(in_idx[i]) > 0:\n",
    "                count_k = counter[i]*1.0\n",
    "                for j in in_idx[i]:\n",
    "                    curr_centroids[i, :] = curr_centroids[i, :]*(1.0 - alpha/count_k) + self.point_mat[j, 0:self.dim_n]*alpha/count_k\n",
    "        return curr_centroids\n",
    "    \n",
    "    '''Exact update of centers'''\n",
    "    def update_centers_kmeans_exact(self, kmeans_batch_idx, arr_label, curr_centroids):\n",
    "        arr_label = arr_label.astype('int') \n",
    "        old_label = arr_label[kmeans_batch_idx]\n",
    "\n",
    "        bacth_dist = self.distance(self.point_mat[kmeans_batch_idx, 0:self.dim_n], curr_centroids)\n",
    "        new_label = np.argmin(bacth_dist, axis = 1) + 1\n",
    "\n",
    "        old_count = collections.Counter(arr_label)\n",
    "        arr_label[kmeans_batch_idx] = new_label\n",
    "        new_count = collections.Counter(arr_label)\n",
    "\n",
    "        in_idx = defaultdict(list)\n",
    "        out_idx = defaultdict(list)\n",
    "        \n",
    "        for i in range(len(kmeans_batch_idx)):\n",
    "            if old_label[i] != new_label[i]:\n",
    "                in_idx[new_label[i]] += [kmeans_batch_idx[i]]\n",
    "                out_idx[old_label[i]] += [kmeans_batch_idx[i]]\n",
    "\n",
    "        for i in range(self.n_clusters):\n",
    "            if len(out_idx[i]) > 0 and len(in_idx[i]) > 0:\n",
    "                curr_centroids[i, :] = curr_centroids[i, :]*old_count[i]/new_count[i] + (np.sum(self.point_mat[in_idx[i], 0:self.dim_n], axis = 0) - np.sum(self.point_mat[out_idx[i], 0:self.dim_n], axis = 0))/new_count[i]\n",
    "            elif len(out_idx[i]) > 0:\n",
    "                curr_centroids[i, :] = (curr_centroids[i, :]*old_count[i] - np.sum(self.point_mat[out_idx[i], 0:self.dim_n], axis = 0))/new_count[i]\n",
    "            elif len(in_idx[i]) > 0:\n",
    "                curr_centroids[i, :] = (curr_centroids[i, :]*old_count[i] + np.sum(self.point_mat[in_idx[i], 0:self.dim_n], axis = 0))/new_count[i]\n",
    "\n",
    "        return curr_centroids, arr_label\n",
    "\n",
    "    def update_centers_balance_exact(self, old_label, arr_label, curr_centroids, in_idx, out_idx):\n",
    "        '''\n",
    "        =========== Input ================\n",
    "        old_label: old assignment before swap (this is to update balance in-time and avoid jump around)\n",
    "        out_idx: index of out points for all clusters\n",
    "        in_idx: index of in points for all clusters\n",
    "        curr_centroids: dictionary of the set of k centers \n",
    "        arr_label： current assignment label vector\n",
    "        =========== Output ================\n",
    "        centroids: new center dictionary\n",
    "        '''\n",
    "        old_count = collections.Counter(old_label)\n",
    "        new_count = collections.Counter(arr_label)\n",
    "        for i in range(self.n_clusters):\n",
    "            if len(out_idx[i]) > 0 and len(in_idx[i]) > 0:\n",
    "                curr_centroids[i, :] = curr_centroids[i, :]*old_count[i]/new_count[i] + (np.sum(self.point_mat[in_idx[i], 0:self.dim_n], axis = 0) - np.sum(self.point_mat[out_idx[i], 0:self.dim_n], axis = 0))/new_count[i]\n",
    "            elif len(out_idx[i]) > 0:\n",
    "                curr_centroids[i, :] = (curr_centroids[i, :]*old_count[i] - np.sum(self.point_mat[out_idx[i], 0:self.dim_n], axis = 0))/new_count[i]\n",
    "            elif len(in_idx[i]) > 0:\n",
    "                curr_centroids[i, :] = (curr_centroids[i, :]*old_count[i] + np.sum(self.point_mat[in_idx[i], 0:self.dim_n], axis = 0))/new_count[i]\n",
    "        return curr_centroids\n",
    "    \n",
    "    ''' Batch update of centers for pure k-means updates'''\n",
    "    def update_centers_kmeans_batch(self, batch_mat, arr_label, curr_centroids, count_demo, alpha):\n",
    "        counter = np.sum(count_demo, axis = 1)\n",
    "        bacth_dist = self.distance(batch_mat, curr_centroids)\n",
    "        closest_center_idx = np.argmin(bacth_dist, axis = 1)\n",
    "\n",
    "        for i in range(self.n_clusters):\n",
    "            index = np.where(closest_center_idx == i)[0]\n",
    "            if len(index) > 0:\n",
    "                counter[i] += len(index)\n",
    "                aver_direction = np.sum(curr_centroids[i].reshape(1, -1) - batch_mat[index,:], axis = 0)\n",
    "                curr_centroids[i, :] = curr_centroids[i, :] - aver_direction/counter[i]*1.0\n",
    "        return curr_centroids, closest_center_idx\n",
    "\n",
    "    ''' Batch update of centers for pure swap updates'''\n",
    "    def update_centers_balance_batch(self, arr_label, curr_centroids, in_idx, count_demo, alpha):\n",
    "        '''\n",
    "        Input\n",
    "        arr_label：vector of clustering labels\n",
    "        curr_centroids: the set of k centers \n",
    "        in_idx: index of in points for all clusters\n",
    "        count_demo: current demographic compositions for k clusters before swap\n",
    "        alpha: 1.0 by default, use to control the step size of center updates\n",
    "        \n",
    "        Output\n",
    "        curr_centroids: the new set of k centers \n",
    "        '''\n",
    "\n",
    "        counter = np.sum(count_demo, axis = 1)\n",
    "        for i in range(self.n_clusters):\n",
    "            if len(in_idx[i]) > 0:\n",
    "                aver_direction = np.sum(curr_centroids[i].reshape(1, -1) - self.point_mat[in_idx[i], 0:self.dim_n], axis = 0)\n",
    "                curr_centroids[i] = curr_centroids[i] - aver_direction/counter[i]*1.0\n",
    "\n",
    "        return curr_centroids\n",
    "    \n",
    "    ## choose point(s) that is closest to the target cluster to swap\n",
    "    def swap_index_selection(self, candidates_idx, num_batch, num_res, label, centroids):\n",
    "        batch_idx = np.random.choice(candidates_idx, num_batch)\n",
    "        batch_mat = self.point_mat[batch_idx, 0:self.dim_n]\n",
    "        arr_dist = np.linalg.norm(batch_mat - centroids[label, :], axis = 1)\n",
    "        return batch_idx[np.argmin(arr_dist)]\n",
    "    \n",
    "    \n",
    "    def batch_fair_kmeans_descent(self, med = 'global', kmean_batch = 4, max_swap = 2, max_iters = 500):\n",
    "        '''\n",
    "        input:\n",
    "            med: methods for choosing the target cluster. Available choice: \"global\"(by default), \"local\".\n",
    "            kmean_batch: number of k-means updates in each outer iteration\n",
    "            max_swap: number of swap updates in each outer iteration\n",
    "            max_iters: total number of outer iterations\n",
    "            \n",
    "        output:\n",
    "            kmean_f1: the sequence of k-means costs using updated centers\n",
    "            kmean_f1_lables: the sequence of k-means cost using the exact centers computed by current labels\n",
    "            balance_f2: the sequence of balances\n",
    "            arr_curr_centroids: the sequence of centers\n",
    "            arr_curr_label: the sequence of labels\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        alpha = 1.0\n",
    "        eps = 1e-15\n",
    "        \n",
    "        curr_label = self.arr_init_labels[0, :] + 0\n",
    "        curr_centroids = self.arr_init_centroids[0, :, :]*1.0\n",
    "\n",
    "        kmean_f1 = np.zeros(max_iters + 1)\n",
    "        kmean_f1_lables = np.zeros(max_iters + 1)\n",
    "        balance_f2 = np.zeros(max_iters + 1)\n",
    "        arr_curr_centroids = np.zeros((max_iters + 1, self.n_clusters, self.dim_n))\n",
    "        arr_curr_label = np.zeros((max_iters + 1, self.n_samples))\n",
    "        \n",
    "        arr_curr_centroids[0, :, :] = curr_centroids\n",
    "        arr_curr_label[0, :] = curr_label\n",
    "        \n",
    "        \n",
    "        self.count_clu = collections.Counter(curr_label)\n",
    "        \n",
    "        kmean_f1[0] = self.update_objective_f1(curr_label, curr_centroids)\n",
    "        kmean_f1_lables[0] = self.update_objective_f1_trueCenter(curr_label)\n",
    "        \n",
    "        count_demo = self.compute_demo(curr_label)\n",
    "        _, balance_f2[0], bn_cluster, bn_low, bn_high = self.compute_balance(count_demo)\n",
    "        biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroids, method = med)\n",
    "\n",
    "        kmean_epoch_idx = np.random.permutation(self.n_samples)\n",
    "        kmean_step_count = 0\n",
    "\n",
    "        swap_indicator_vector = np.zeros([self.n_clusters, self.n_samples])\n",
    "        reset_count = 0\n",
    "        \n",
    "        \n",
    "        iters = 1\n",
    "        while iters <= max_iters:\n",
    "            \n",
    "            ## do max_swap steps of pure swap updates\n",
    "            num_batch = min(self.n_samples, int(1.005**(iters // 2)))\n",
    "            swap_success_count = 0\n",
    "            in_idx = defaultdict(list)\n",
    "            out_idx = defaultdict(list)\n",
    "            old_label = curr_label + 0\n",
    "            \n",
    "            if max_swap > 0:\n",
    "                ## check if the minimum balanced cluster changes or not\n",
    "                if np.argmin(np.min(count_demo, axis = 1)/np.max(count_demo, axis = 1)) != bn_cluster:\n",
    "                    _, balance, bn_cluster, bn_low, bn_high = self.compute_balance(count_demo)\n",
    "                else:\n",
    "                    bn_low = np.argmin(count_demo[bn_cluster, :])\n",
    "                    bn_high = np.argmax(count_demo[bn_cluster, :])\n",
    "                biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroids, method = med)\n",
    "            \n",
    "            while swap_success_count < max_swap:\n",
    "                ## check if the target well-balanced cluster changes or not\n",
    "                if swap_success_count > 0:\n",
    "                    if med == 'global':\n",
    "                        if np.argmax(count_demo[:, bn_low]/(count_demo[:, bn_high] + eps)) != biased_cluster:\n",
    "                            biased_cluster = np.argmax(count_demo[:, bn_low]/(count_demo[:, bn_high] + eps))\n",
    "                    elif med == 'local':\n",
    "                        biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroids, method = med)\n",
    "                \n",
    "                candidates_idx2 = np.where((self.gender == bn_low) & (curr_label == biased_cluster) & (swap_indicator_vector[biased_cluster, :] == 0))[0] \n",
    "                if len(candidates_idx2) == 0:\n",
    "                    swap_indicator_vector[biased_cluster, :] = np.zeros([1, self.n_samples])\n",
    "                    reset_count += 1\n",
    "                    candidates_idx2 = np.where((self.gender == bn_low) & (curr_label == biased_cluster))[0] \n",
    "                    \n",
    "                idx_2 = self.swap_index_selection(candidates_idx2, min(num_batch, len(candidates_idx2)), 1, bn_cluster, curr_centroids) # np.random.choice(candidates_idx2, 1)\n",
    "                in_idx[bn_cluster] += [idx_2] \n",
    "                out_idx[biased_cluster] += [idx_2]\n",
    "                curr_label[idx_2] = bn_cluster\n",
    "\n",
    "                count_demo[bn_cluster, bn_low] += 1\n",
    "                count_demo[biased_cluster, bn_low] -= 1\n",
    "\n",
    "                swap_success_count += 1\n",
    "                swap_indicator_vector[bn_cluster, idx_2] = 1\n",
    "                swap_indicator_vector[biased_cluster, idx_2] = 1\n",
    "                \n",
    "                if swap_success_count < max_swap:\n",
    "                    ## check if the minimum balanced cluster changes or not\n",
    "                    if np.argmin(np.min(count_demo, axis = 1)/np.max(count_demo, axis = 1)) != bn_cluster:\n",
    "                        _, balance, bn_cluster, bn_low, bn_high = self.compute_balance(count_demo)\n",
    "                    else:\n",
    "                        bn_low = np.argmin(count_demo[bn_cluster, :])\n",
    "                        bn_high = np.argmax(count_demo[bn_cluster, :])\n",
    "                    biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroids, method = med)\n",
    "                    \n",
    "                    candidates_idx1 = np.where((self.gender == bn_high) & (curr_label == bn_cluster) & (swap_indicator_vector[bn_cluster, :] == 0))[0]\n",
    "                    if len(candidates_idx1) == 0:\n",
    "                        swap_indicator_vector[bn_cluster, :] = np.zeros([1, self.n_samples])\n",
    "                        reset_count += 1\n",
    "                        candidates_idx1 = np.where((self.gender == bn_high) & (curr_label == bn_cluster))[0]\n",
    "                    \n",
    "                    idx_1 = self.swap_index_selection(candidates_idx1, min(num_batch, len(candidates_idx1)), 1, biased_cluster, curr_centroids) #np.random.choice(candidates_idx1, 1)\n",
    "                    in_idx[biased_cluster] += [idx_1]\n",
    "                    out_idx[bn_cluster] += [idx_1]\n",
    "                    curr_label[idx_1] = biased_cluster\n",
    "\n",
    "                    count_demo[biased_cluster, bn_high] += 1\n",
    "                    count_demo[bn_cluster, bn_high] -= 1\n",
    "\n",
    "                    swap_success_count += 1\n",
    "                    swap_indicator_vector[bn_cluster, idx_1] = 1\n",
    "                    swap_indicator_vector[biased_cluster, idx_1] = 1\n",
    "\n",
    "            if max_swap > 0:    \n",
    "                curr_centroids = self.update_centers_balance_batch(curr_label, curr_centroids, in_idx, count_demo, alpha)\n",
    "    \n",
    "            ## do kmean_batch steps of pure kmeans updates\n",
    "            if kmean_batch > 0:\n",
    "                if kmean_batch + kmean_step_count > self.n_samples:\n",
    "                    kmeans_batch_idx = kmean_epoch_idx[kmean_step_count:]\n",
    "                    kmean_epoch_idx = np.random.permutation(self.n_samples) ## re-shuffle\n",
    "                    kmean_step_count = 0\n",
    "                else:\n",
    "                    kmeans_batch_idx = kmean_epoch_idx[kmean_step_count:kmean_batch + kmean_step_count]\n",
    "                    kmean_step_count += kmean_batch\n",
    "                    \n",
    "                point_mat_batch = self.point_mat[kmeans_batch_idx, :self.dim_n] \n",
    "                old_label_batch = curr_label[kmeans_batch_idx]\n",
    "#                 curr_centroids, batch_center_idx = self.update_centers_kmeans(point_mat_batch, curr_label, curr_centroids, alpha)\n",
    "                curr_centroids, batch_center_idx = self.update_centers_kmeans_batch(point_mat_batch, curr_label, curr_centroids, count_demo, alpha)\n",
    "                curr_label[kmeans_batch_idx] = batch_center_idx\n",
    "        \n",
    "                for demo, old, new in zip(self.point_mat[kmeans_batch_idx, self.dim_n], old_label_batch, batch_center_idx):\n",
    "                    if old != new:\n",
    "                        count_demo[int(new), int(demo)] += 1\n",
    "                        count_demo[int(old), int(demo)] -= 1 \n",
    "    \n",
    "            kmean_f1[iters] = self.update_objective_f1(curr_label, curr_centroids)\n",
    "            _, balance_f2[iters], _ , _, _ = self.compute_balance(count_demo)\n",
    "\n",
    "            kmean_f1_lables[iters] = self.update_objective_f1_trueCenter(curr_label)\n",
    "            \n",
    "            arr_curr_centroids[iters, :, :] = curr_centroids\n",
    "            arr_curr_label[iters, :] = curr_label\n",
    "            \n",
    "            iters += 1\n",
    "        print(\"Total number of outer iterations:\", iters)\n",
    "        return kmean_f1, kmean_f1_lables, balance_f2, arr_curr_centroids, arr_curr_label\n",
    "    \n",
    "    def stochastic_alt_descent(self, labels, centroids, count_demo, kmean_batch = 50, max_swap = 2, num_iters = 2, med = 'global'):\n",
    "        '''\n",
    "        do num_iters times alternative k-means and swap updates from the given clustering label and center.\n",
    "        \n",
    "        input:\n",
    "            labels: given initial labels\n",
    "            centroids: given initial centers\n",
    "            count_demo: initial demographic compositions for k clusters\n",
    "            kmean_batch: n_a, number of k-means updates per iteration\n",
    "            max_swap: n_b, number of swap updates per iteration\n",
    "            num_iters: number of iterations\n",
    "            med: methods for choosing the target cluster. Available choice: \"global\"(by default), \"local\".\n",
    "            \n",
    "        output:\n",
    "            kmean_f1: updated k-means cost using updated centers\n",
    "            balance_f2: updated balance\n",
    "            curr_centroid: updated centers\n",
    "            curr_label: updated labels\n",
    "            count_demo: updated demographic compositions\n",
    "            kmean_f1_lables: updated k-means cost using the exact centers computed by current labels\n",
    "            kmean_f1_centers: updated k-means cost using the exact clustering labels computed by current centers\n",
    "\n",
    "        '''\n",
    "        curr_label = labels+0\n",
    "        curr_centroid = centroids+0\n",
    "        eps = 1e-15\n",
    "        \n",
    "        _, balance, bn_cluster, bn_low, bn_high = self.compute_balance(count_demo)\n",
    "        biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroid, method = med)\n",
    "        num_batch = min(self.n_samples, int(1.006**(self.num_iter // 2)))\n",
    "        \n",
    "        for t in range(num_iters):\n",
    "            \n",
    "            ## do certain steps of pure swap updates\n",
    "            swap_success_count = 0\n",
    "            in_idx = defaultdict(list)\n",
    "            out_idx = defaultdict(list)\n",
    "            old_label = curr_label + 0\n",
    "            \n",
    "            if max_swap > 0:\n",
    "                ## check if the minimum balanced cluster changes or not\n",
    "                if np.argmin(np.min(count_demo, axis = 1)/np.max(count_demo, axis = 1)) != bn_cluster:\n",
    "                    _, balance, bn_cluster, bn_low, bn_high = self.compute_balance(count_demo)\n",
    "                else:\n",
    "                    bn_low = np.argmin(count_demo[bn_cluster, :])\n",
    "                    bn_high = np.argmax(count_demo[bn_cluster, :])\n",
    "                biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroid, method = med)\n",
    "\n",
    "\n",
    "            while swap_success_count < max_swap:\n",
    "                \n",
    "                ## check if the target cluster changes or not\n",
    "                if swap_success_count > 0:\n",
    "                    if med == 'global':\n",
    "                        if np.argmax(count_demo[:, bn_low]/(count_demo[:, bn_high] + eps)) != biased_cluster:\n",
    "                            biased_cluster = np.argmax(count_demo[:, bn_low]/(count_demo[:, bn_high] + eps))\n",
    "                    elif med == 'local':\n",
    "                        biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroid, method = med)\n",
    "                \n",
    "                candidates_idx2 = np.where((self.gender == bn_low) & (curr_label == biased_cluster))[0] \n",
    "                idx_2 = self.swap_index_selection(candidates_idx2, min(num_batch, len(candidates_idx2)), 1, bn_cluster, curr_centroid) # np.random.choice(candidates_idx2, 1)\n",
    "                in_idx[bn_cluster] += [idx_2] \n",
    "                out_idx[biased_cluster] += [idx_2]\n",
    "                curr_label[idx_2] = bn_cluster\n",
    "\n",
    "                count_demo[bn_cluster, bn_low] += 1\n",
    "                count_demo[biased_cluster, bn_low] -= 1\n",
    "\n",
    "                swap_success_count += 1\n",
    "                \n",
    "                if swap_success_count < max_swap:\n",
    "                    ## check if the minimum balanced cluster changes or not\n",
    "                    if np.argmin(np.min(count_demo, axis = 1)/np.max(count_demo, axis = 1)) != bn_cluster:\n",
    "                        _, balance, bn_cluster, bn_low, bn_high = self.compute_balance(count_demo)\n",
    "                    else:\n",
    "                        bn_low = np.argmin(count_demo[bn_cluster, :])\n",
    "                        bn_high = np.argmax(count_demo[bn_cluster, :])\n",
    "                    biased_cluster = self.choose_target_cluster(bn_cluster, bn_low, bn_high, count_demo, curr_centroid, method = med)\n",
    "                    \n",
    "\n",
    "                    candidates_idx1 = np.where((self.gender == bn_high) & (curr_label == bn_cluster))[0]\n",
    "                    \n",
    "                    idx_1 = self.swap_index_selection(candidates_idx1, min(num_batch, len(candidates_idx1)), 1, biased_cluster, curr_centroid) #np.random.choice(candidates_idx1, 1)\n",
    "                    in_idx[biased_cluster] += [idx_1]\n",
    "                    out_idx[bn_cluster] += [idx_1]\n",
    "                    curr_label[idx_1] = biased_cluster\n",
    "\n",
    "                    count_demo[biased_cluster, bn_high] += 1\n",
    "                    count_demo[bn_cluster, bn_high] -= 1\n",
    "\n",
    "                    swap_success_count += 1\n",
    "\n",
    "            if max_swap > 0:\n",
    "                curr_centroid = self.update_centers_balance_batch(curr_label, curr_centroid, in_idx, count_demo, self.stepsize)\n",
    "\n",
    "             ## do certain steps of pure kmeans updates\n",
    "            if kmean_batch > 0:\n",
    "                kmeans_batch_idx = np.random.choice(self.n_samples, kmean_batch, replace=False)\n",
    "                point_mat_batch = self.point_mat[kmeans_batch_idx, 0:self.dim_n] \n",
    "                old_label_batch = curr_label[kmeans_batch_idx]\n",
    "                curr_centroid, batch_center_idx = self.update_centers_kmeans_batch(point_mat_batch, curr_label, curr_centroid, count_demo, self.stepsize)\n",
    "                curr_label[kmeans_batch_idx] = batch_center_idx\n",
    "                \n",
    "                ## update demo composition for k clusters\n",
    "                for demo, old, new in zip(self.point_mat[kmeans_batch_idx, self.dim_n], old_label_batch, batch_center_idx):\n",
    "                    if old != new:\n",
    "                        count_demo[int(new), int(demo)] += 1\n",
    "                        count_demo[int(old), int(demo)] -= 1\n",
    "        \n",
    "        kmean_f1 = 0 #self.update_objective_f1(curr_label, curr_centroid)\n",
    "        _, balance_f2,_, _, _ = self.compute_balance(count_demo)\n",
    "\n",
    "        kmean_f1_lables = self.update_objective_f1_trueCenter(curr_label)\n",
    "        kmean_f1_centers= 0 #self.update_objective_f1_kmeans(curr_centroid)\n",
    "\n",
    "        return kmean_f1, balance_f2, curr_centroid, curr_label, count_demo, kmean_f1_lables, kmean_f1_centers\n",
    "    \n",
    "    # remove non-dominated points from current list from dense region\n",
    "    def remove_dense(self, list_f1, list_f2, list_centers, list_labels, list_demo):\n",
    "        num_total_pts = len(list_f1)\n",
    "        if num_total_pts <= 50:\n",
    "            return list_f1, list_f2, list_centers, list_labels, list_demo\n",
    "        if self.dense_threshold == 0:\n",
    "            dense_threshold = 1.0/(200 + self.num_iter/2.0) # 1.0/(100 + self.num_iter/2.0) good for bank\n",
    "        else:\n",
    "            dense_threshold = self.dense_threshold\n",
    "    \n",
    "        index_f1 = np.argsort(list_f1)\n",
    "        index_f2 = np.argsort(list_f2)\n",
    "        temp_list_f1 = np.sort(list_f1)\n",
    "        temp_list_f2 = np.sort(list_f2)\n",
    "        \n",
    "        min_gap_f1 = (temp_list_f1[-1] - temp_list_f1[0])*dense_threshold + 1e-16\n",
    "        max_gap_f2 = (temp_list_f2[-1] - temp_list_f2[0])*dense_threshold + 1e-16\n",
    "\n",
    "        diff_list_f1 = np.diff(temp_list_f1)\n",
    "        diff_list_f2 = np.diff(temp_list_f2)\n",
    "        keep_index_f1 = [0, num_total_pts - 1]\n",
    "        keep_index_f2 = [0, num_total_pts - 1]\n",
    "        \n",
    "        i = 1\n",
    "        while i < num_total_pts - 2: # keep the first and last ones\n",
    "            j = i\n",
    "            curr_gap = diff_list_f1[j]\n",
    "            while curr_gap <= min_gap_f1 and j < num_total_pts - 2:\n",
    "                j += 1\n",
    "                curr_gap += diff_list_f1[j]\n",
    "            keep_index_f1.append((j - i)//2 + i)\n",
    "\n",
    "            if i < j:\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        i = 1\n",
    "        while i < num_total_pts - 2: # keep the first and last ones\n",
    "            j = i\n",
    "            curr_gap = diff_list_f2[j]\n",
    "            while curr_gap <= max_gap_f2 and j < num_total_pts - 2:\n",
    "                j += 1\n",
    "                curr_gap += diff_list_f2[j]\n",
    "            keep_index_f2.append((j - i)//2 + i)\n",
    "\n",
    "            if i < j:\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        keep_index_f1 = np.unique(keep_index_f1)\n",
    "        keep_index_f2 = np.unique(keep_index_f2)\n",
    "        keep_index = np.union1d(index_f1[keep_index_f1], index_f2[keep_index_f2])\n",
    "        \n",
    "        return list_f1[keep_index], list_f2[keep_index], list_centers[keep_index, :, :], \\\n",
    "    list_labels[keep_index, :], list_demo[keep_index, :, :]\n",
    "    \n",
    "    # Remove dominated points at the end of each iteration\n",
    "    def clear(self, list_f1, list_f2, list_centers, list_labels, list_demo):\n",
    "        num_total_pts = len(list_f1)\n",
    "        if num_total_pts <= 50:\n",
    "            return list_f1, list_f2, list_centers, list_labels, list_demo\n",
    "        \n",
    "        array_f1 = list_f1\n",
    "        array_f2 = -list_f2 \n",
    "        \n",
    "        x_bar = np.repeat(array_f1.reshape(-1,1), len(list_f1),axis = 1)\n",
    "        y_bar = np.repeat(array_f2.reshape(-1,1), len(list_f2),axis = 1)\n",
    "        \n",
    "        x_check1 = (array_f1 <= x_bar)\n",
    "        x_check2 = (array_f1 < x_bar)\n",
    "        \n",
    "        y_check1 = (array_f2 <= y_bar)\n",
    "        y_check2 = (array_f2 < y_bar)\n",
    "        \n",
    "        all_check1 = (x_check1 & y_check2)\n",
    "        all_check2 = (x_check2 & y_check1)\n",
    "        \n",
    "        sum1 = all_check1.sum(axis = 1)\n",
    "        sum2 = all_check2.sum(axis = 1)\n",
    "        \n",
    "        rest_index = np.array([i for i in range(len(list_f1)) if (sum1[i] < 1 or sum2[i] < 1)])\n",
    "        \n",
    "        return array_f1[rest_index], -array_f2[rest_index], list_centers[rest_index, :, :], \\\n",
    "    list_labels[rest_index, :], list_demo[rest_index, :, :]\n",
    "    \n",
    "    def main_PFSAD(self, seed = 1):\n",
    "        '''\n",
    "        main function for Pareto front version of SAfairKM algorithms\n",
    "        \n",
    "        seed: random seed for generating initial labels/centers\n",
    "        '''\n",
    "        if self.init_provided: ## run the algorithm from an intermediate non-dominated point list\n",
    "            f1_value_list, f2_value_list = self.f1_list, self.f2_list \n",
    "        else:\n",
    "            self.num_iter = 0\n",
    "            \n",
    "            ## call init functions to initialize a list of labels and centers\n",
    "            self.init_labels_centers(seed)\n",
    "            f1_value_list = np.zeros(self.n_init)\n",
    "            f2_value_list = np.zeros(self.n_init)\n",
    "            demo_mat_list = np.zeros((self.n_init, self.n_clusters, self.n_demoGroups))\n",
    "\n",
    "            for i in range(self.n_init):\n",
    "    #             f1_value_list[i] = self.update_objective_f1(self.arr_init_labels[i, :], self.arr_init_centroids[i, :, :])\n",
    "                f1_value_list[i] = self.update_objective_f1_trueCenter(self.arr_init_labels[i, :])\n",
    "                demo_mat_list[i, :, :] = self.compute_demo(self.arr_init_labels[i, :])\n",
    "                _, f2_value_list[i], _, _, _ = self.compute_balance(demo_mat_list[i, :, :])\n",
    "        \n",
    "        updating_label_list = self.arr_init_labels \n",
    "        updating_center_list = self.arr_init_centroids\n",
    "        \n",
    "        if self.data_name in ['Adult', 'Bank']:\n",
    "            pairs = [(25, 0), (20, 4), (15, 3), (5, 2)]\n",
    "            num_pairs = 4\n",
    "        else:\n",
    "            pairs = [(1, 1), (5, 1), (2, 0)]\n",
    "            num_pairs = 3\n",
    "        \n",
    "        np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "        begin = time.time()\n",
    "        while len(updating_label_list) < self.max_len_pareto_front and self.num_iter < self.max_iter:\n",
    "            tic = time.time()\n",
    "\n",
    "            ## Allocate memory for shared + repeated data \n",
    "            shared_updating_label_list = np.tile(updating_label_list, (self.point_per_iteration, 1))\n",
    "            shared_updating_center_list = np.tile(updating_center_list, (self.point_per_iteration, 1, 1))\n",
    "            shared_demo_mat_list = np.tile(demo_mat_list, (self.point_per_iteration, 1, 1))\n",
    "\n",
    "            pool = multiprocessing.Pool(processes=16)\n",
    "            zipped_res1 = pool.starmap(partial(self.stochastic_alt_descent, kmean_batch = 5, max_swap = 1), zip(shared_updating_label_list, shared_updating_center_list, shared_demo_mat_list))\n",
    "            zipped_res2 = pool.starmap(partial(self.stochastic_alt_descent, kmean_batch = 2, max_swap = 0), zip(shared_updating_label_list, shared_updating_center_list, shared_demo_mat_list))\n",
    "            zipped_res3 = pool.starmap(partial(self.stochastic_alt_descent, kmean_batch = 1, max_swap = 1), zip(shared_updating_label_list, shared_updating_center_list, shared_demo_mat_list))\n",
    "            zipped_stacked_res = np.asarray(np.vstack(zipped_res1 + zipped_res2 + zipped_res3))\n",
    "            \n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            pool.terminate()\n",
    "            \n",
    "            f1_value_list = np.append(f1_value_list, zipped_stacked_res[:, 5])\n",
    "            f2_value_list = np.append(f2_value_list, zipped_stacked_res[:, 1])\n",
    "            updating_center_list = np.vstack([updating_center_list, np.stack(zipped_stacked_res[:, 2])])\n",
    "            updating_label_list = np.vstack([updating_label_list, np.stack(zipped_stacked_res[:, 3])])\n",
    "            demo_mat_list = np.vstack([demo_mat_list, np.stack(zipped_stacked_res[:, 4])])\n",
    "\n",
    "\n",
    "            f1_value_list, f2_value_list, updating_center_list, updating_label_list, demo_mat_list = self.clear(f1_value_list, f2_value_list, updating_center_list, updating_label_list, demo_mat_list)\n",
    "            f1_value_list, f2_value_list, updating_center_list, updating_label_list, demo_mat_list = self.remove_dense(f1_value_list, f2_value_list, updating_center_list, updating_label_list, demo_mat_list)\n",
    "            \n",
    "            print(\"time: \",  time.time() - tic)\n",
    "            self.num_iter += self.num_steps_per_point\n",
    "            \n",
    "            print(\"#Pts: \", len(f1_value_list), \" #Iter: \", self.num_iter)\n",
    "            \n",
    "        total_time = time.time() - begin\n",
    "        print(\"Total time: \",  total_time)\n",
    "        \n",
    "        return f1_value_list, f2_value_list, updating_center_list, updating_label_list, total_time \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d97a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
